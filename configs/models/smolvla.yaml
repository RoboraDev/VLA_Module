# SmolVLA Model Configuration
# Usecase: A Small & highly efficient VLA Model for robotics

model:
  name: "smolvla"
  display_name: "SmolVLA"
  version: "base"           # Vanilla version
  description: "SmolVLA: A vision-language-action model for affordable and efficient robotics"
  
  # Model specifications
  architecture:
    type: "vision-language-action"
    backbone: "SmolVLM2"
    parameters: "450M"
    vision_encoder: "SmolVLM2 Vision Component"
    language_model: "SmolVLM2 Language Component" 
    action_expert: "Flow Matching Action Head"
    total_size_gb: 0.9                                      # Exact model weights size, inference size may be different.
    
  # Hugging Face Hub details
  hub:
    repo_id: "lerobot/smolvla_base"
    model_type: "lerobot"
    library: "lerobot"
    pipeline_tag: "robotics"
    license: "apache-2.0"
    tags: ["robotics", "vision-language-action", "lerobot", "smolvla"]
    
  # Model capabilities and features
  capabilities:
    vision:
      - "Multi-camera input support"
      - "Real-time image processing"
      - "Scene understanding"
    language:
      - "Natural language instruction following"
      - "Task description parsing"
      - "Context awareness"
    action:
      - "Flow matching action generation"
      - "Real-time control (50Hz capable)"
      - "Action chunk prediction"
      - "Continuous action spaces"
    
  # Performance characteristics
  performance:
    inference_speed: "Real-time"
    action_frequency: "50Hz"
    memory_usage: "Low (consumer hardware compatible)"
    gpu_requirements: "Optional (CPU inference possible)"
    recommended_gpu: "GTX 1060 or higher"
    
  # Training and fine-tuning
  training:
    pretrained_weights: true
    fine_tuning_strategies:
      - "Action head fine-tuning (recommended)"
      - "Full model fine-tuning"
    recommended_episodes: 50
    training_time_single_a100: "~4 hours (20k steps)"
    batch_size_default: 64
    learning_rate_default: 1e-4
    
  # Input/Output specifications  
  input:
    image:
      format: ["RGB", "RGBD"]
      resolution: [224, 224]  # Typical vision transformer input
      channels: [3, 4]  # RGB or RGBD
      preprocessing: "Standard normalization"
    instruction:
      type: "text"
      max_length: 512  # Typical for language models
      encoding: "tokenized"
    
    
    # Important parameter I need to take special care about! # TODO-1 for Shiven make sure the proprioception is taken care of inside sim environment
    # Add some random sensor noise and irregularities so, the model can have proper sim to real abilities transfer.
    proprioception:
      joint_positions: true
      joint_velocities: true
      end_effector_pose: true
      
  output:
    action:
      type: "continuous"
      dimensions: 7  # Trained on https://huggingface.co/datasets/lerobot/svla_so101_pickplace 
      format: "action_chunk"
      chunk_size: 8  # 7 FOR JOINTS AND 8th ONE FOR GRIPPER STATUS (OPEN/CLOSE)
      prediction_horizon: "Multi-step"          # Flow matching makes the action transformer generate action chunks in parallel.
      
  # Dependencies and requirements for PyTorch with CUDA 12.8 Runtime.
  dependencies:
    core:
      - "lerobot>=0.3.2"
      - "torch>=2.8.0"
      - "torchvision>=0.23.0" 
      - "transformers>=4.56.2"
      - "numpy>=2.3.3"
    optional:
      - "wandb"  # For training visualization
      - "opencv-python"  # For camera input
      - "pillow"  # For image processing
      
  # Installation and setup
  installation:
    command: 'pip install -e ".[smolvla]"'
    extra_dependencies: "smolvla"
    environment_setup: "LeRobot installation required"
    
  # Usage examples and commands
  usage:
    download:
      cli: "uv run vla-cli download --model smolvla --cache-dir ./models"
      python: |
        from vla_module.models import load_model
        model = load_model("smolvla", device="cuda")
        
    inference:
      cli: 'uv run vla-cli infer --model smolvla --image path/to/image.jpg --command "pick up the red cube"'
      python: |
        from vla_module.models.inference import VLAInference
        inference_engine = VLAInference(model)
        actions = inference_engine.predict(image_path="observation.jpg", command="move forward")
        
    training:
      fine_tune_action_head: |
        lerobot-train \
          --policy.path=lerobot/smolvla_base \
          --dataset.repo_id=your_dataset \
          --batch_size=64 \
          --steps=20000 \
          --output_dir=outputs/train/my_smolvla \
          --policy.device=cuda
          
      train_from_scratch: |
        lerobot-train \
          --dataset.repo_id=your_dataset \
          --batch_size=64 \
          --steps=200000 \
          --output_dir=outputs/train/my_smolvla \
          --policy.device=cuda
          
    api_server:
      start: "uv run vla-cli serve --model smolvla --host 0.0.0.0 --port 8000"
      endpoint: "POST /inference"
      
  # Documentation and resources
  resources:
    paper: "https://huggingface.co/papers/2506.01844"
    blog_post: "https://huggingface.co/blog/smolvla"
    documentation: "https://huggingface.co/docs/lerobot/smolvla"
    code: "https://github.com/huggingface/lerobot/blob/main/lerobot/common/policies/smolvla/modeling_smolvla.py"
    colab_training: "https://colab.research.google.com/github/huggingface/notebooks/blob/main/lerobot/training-smolvla.ipynb"
    model_hub: "https://huggingface.co/lerobot/smolvla_base"
    example_dataset: "https://huggingface.co/datasets/lerobot/svla_so101_pickplace"
    
  # Compatibility and integration
  compatibility:
    frameworks: ["LeRobot", "PyTorch", "Transformers"]
    robots:
      - "SO101 Follower Arm"
      - "Universal robot arms (7-DOF)"
      - "Custom robot platforms (with fine-tuning)"
    cameras: ["RGB cameras", "RGBD cameras", "Multi-camera setups"]
    environments: ["Real-world robotics", "Simulation (PyBullet)", "Mixed reality"]
    
  # Evaluation and benchmarking  
  evaluation:
    datasets:
      - "SVLA SO100 PickPlace"
      - "Custom task datasets"
    metrics:
      - "Success rate"
      - "Task completion time"
      - "Action accuracy"
      - "Language following rate"
    benchmark_performance:
      pick_place_success: "High (with fine-tuning)"
      language_following: "Excellent"
      generalization: "Good (with sufficient training data)"
      
  # Configuration overrides for specific use cases
  presets:
    development:
      batch_size: 32
      steps: 10000
      device: "cpu"
      wandb_enabled: false
      
    production:
      batch_size: 64
      steps: 20000
      device: "cuda"
      wandb_enabled: true
      optimization: "torch.compile"
      
    research:
      batch_size: 128
      steps: 50000
      device: "cuda"
      wandb_enabled: true
      detailed_logging: true
      
# Additional metadata
metadata:
  created_by: "VLA_Modules Framework"
  version: "1.0.0"
  last_updated: "2025-09-24"
  status: "active"
  priority: "high"
  category: "vision-language-action"
  
# Notes for developers
notes: |
  SmolVLA is optimized for consumer hardware with 450M parameters.
  It uses flow matching for action generation, providing real-time control capabilities.
  Fine-tuning with ~50 episodes is recommended for good performance on new tasks.
  The model excels at pick-and-place tasks and natural language instruction following.
  Consider using action head fine-tuning for faster training and better stability.

action_head_modules: |
  model.state_proj
  model.action_in_proj
  model.action_out_proj
  model.action_time_mlp_in
  model.action_time_mlp_out
  model.vlm_with_expert.lm_expert

modules: |
  normalize_inputs
  normalize_inputs.buffer_observation_state
  normalize_targets
  normalize_targets.buffer_action
  unnormalize_outputs
  unnormalize_outputs.buffer_action
  model
  model.vlm_with_expert
  model.vlm_with_expert.vlm
  model.vlm_with_expert.vlm.model
  model.vlm_with_expert.vlm.model.vision_model
  model.vlm_with_expert.vlm.model.vision_model.embeddings
  model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding
  model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding
  model.vlm_with_expert.vlm.model.vision_model.encoder
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj                                                                              model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1          model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn        model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1      model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2          model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.activation_fn
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2
  model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2
  model.vlm_with_expert.vlm.model.vision_model.post_layernorm
  model.vlm_with_expert.vlm.model.connector
  model.vlm_with_expert.vlm.model.connector.modality_projection
  model.vlm_with_expert.vlm.model.connector.modality_projection.proj
  model.vlm_with_expert.vlm.model.text_model
  model.vlm_with_expert.vlm.model.text_model.embed_tokens
  model.vlm_with_expert.vlm.model.text_model.layers
  model.vlm_with_expert.vlm.model.text_model.layers.0
  model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.1
  model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.2
  model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.3
  model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.4
  model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.5
  model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.6
  model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.7
  model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.8
  model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.9
  model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.10
  model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.11
  model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.12
  model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.13
  model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.14
  model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.15
  model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn
  model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.mlp
  model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj
  model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.act_fn
  model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm
  model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm
  model.vlm_with_expert.vlm.model.text_model.norm
  model.vlm_with_expert.vlm.model.text_model.rotary_emb
  model.vlm_with_expert.vlm.lm_head
  model.vlm_with_expert.lm_expert
  model.vlm_with_expert.lm_expert.layers
  model.vlm_with_expert.lm_expert.layers.0
  model.vlm_with_expert.lm_expert.layers.0.self_attn
  model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.0.mlp
  model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.0.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.0.input_layernorm
  model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.1
  model.vlm_with_expert.lm_expert.layers.1.self_attn
  model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.1.mlp
  model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.1.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.1.input_layernorm
  model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.2
  model.vlm_with_expert.lm_expert.layers.2.self_attn
  model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.2.mlp
  model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.2.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.2.input_layernorm
  model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.3
  model.vlm_with_expert.lm_expert.layers.3.self_attn
  model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.3.mlp
  model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.3.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.3.input_layernorm
  model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.4
  model.vlm_with_expert.lm_expert.layers.4.self_attn
  model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.4.mlp
  model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.4.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.4.input_layernorm
  model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.5
  model.vlm_with_expert.lm_expert.layers.5.self_attn
  model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.5.mlp
  model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.5.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.5.input_layernorm
  model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.6
  model.vlm_with_expert.lm_expert.layers.6.self_attn
  model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.6.mlp
  model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.6.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.6.input_layernorm
  model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.7
  model.vlm_with_expert.lm_expert.layers.7.self_attn
  model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.7.mlp
  model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.7.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.7.input_layernorm
  model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.8
  model.vlm_with_expert.lm_expert.layers.8.self_attn
  model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.8.mlp
  model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.8.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.8.input_layernorm
  model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.9
  model.vlm_with_expert.lm_expert.layers.9.self_attn
  model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.9.mlp
  model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.9.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.9.input_layernorm
  model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.10
  model.vlm_with_expert.lm_expert.layers.10.self_attn
  model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.10.mlp
  model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.10.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.10.input_layernorm
  model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.11
  model.vlm_with_expert.lm_expert.layers.11.self_attn
  model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.11.mlp
  model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.11.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.11.input_layernorm
  model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.12
  model.vlm_with_expert.lm_expert.layers.12.self_attn
  model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.12.mlp
  model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.12.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.12.input_layernorm
  model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.13
  model.vlm_with_expert.lm_expert.layers.13.self_attn
  model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.13.mlp
  model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.13.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.13.input_layernorm
  model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.14
  model.vlm_with_expert.lm_expert.layers.14.self_attn
  model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.14.mlp
  model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.14.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.14.input_layernorm
  model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm
  model.vlm_with_expert.lm_expert.layers.15
  model.vlm_with_expert.lm_expert.layers.15.self_attn
  model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj
  model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj
  model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj
  model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj
  model.vlm_with_expert.lm_expert.layers.15.mlp
  model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj
  model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj
  model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj
  model.vlm_with_expert.lm_expert.layers.15.mlp.act_fn
  model.vlm_with_expert.lm_expert.layers.15.input_layernorm
  model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm
  model.vlm_with_expert.lm_expert.norm
  model.vlm_with_expert.lm_expert.rotary_emb
  model.state_proj
  model.action_in_proj
  model.action_out_proj
  model.action_time_mlp_in
  model.action_time_mlp_out